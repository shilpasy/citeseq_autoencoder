{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Integrative analysis of single-cell multiomics data using deep learning\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.ipynb)   \n",
    "**Recording:** \n",
    "[![YouTubb](https://img.shields.io/static/v1?logo=youtube&label=&message=Youtube&color=red)](https://youtu.be/ELEqNwv9vkE)   \n",
    "**Author:** Yuan Tian [![Connect](https://img.shields.io/static/v1?label=&logo=linkedin&message=Connect&color=blue)](https://www.linkedin.com/in/ytiancompbio) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will take a closer look at autoencoders (AE). Autoencoders are trained on encoding input data such as images into a smaller feature vector, and afterward, reconstruct it by a second neural network, called a decoder. The feature vector is called the “bottleneck” of the network as we aim to compress the input data into a smaller amount of features. This property is useful in many applications, in particular in compressing data or comparing images on a metric beyond pixel-level comparisons. Besides learning about the autoencoder framework, we will also see the “deconvolution” (or transposed convolution) operator in action for scaling up feature maps in height and width. Such deconvolution networks are necessary wherever we start from a small feature vector and need to output an image of full size (e.g. in VAE, GANs, or super-resolution applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from urllib.error import HTTPError\n",
    "from tqdm.notebook import tqdm \n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Pytorch and Pytorch Lightning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Plotting\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "# Tensorboard extension (for visualization purposes later)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = Path(\"data\")\n",
    "if not DATASET_PATH.exists():\n",
    "    DATASET_PATH.mkdir()\n",
    "    \n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = Path(\"saved_models\")\n",
    "if not CHECKPOINT_PATH.exists():\n",
    "    CHECKPOINT_PATH.mkdir()\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-cell multiomics and CITE-seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Single-cell sequencing reveals cellular heterogeneity that is masked by bulk sequencing methods.</li>\n",
    "    <li>CITE-seq simultaneously measures gene expression and surface protein at a single-cell level.</li>\n",
    "</ul>\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"imgs/citeseq.jpg\"/></center>\n",
    "    <center><figcaption>Image source: 10x Genomics</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for downloading data\n",
    "data_url = \"https://raw.githubusercontent.com/naity/citeseq_autoencoder/master/data/\"\n",
    "\n",
    "# Files to download\n",
    "data_files = [\"rna_scale.csv.gz\", \"protein_scale.csv.gz\", \"metadata.csv.gz\"]\n",
    "\n",
    "# Download datafile if necessary\n",
    "for file_name in data_files:\n",
    "    file_path = Path(DATASET_PATH/file_name)\n",
    "    if not file_path.exists():\n",
    "        file_url = data_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try downloading the file from the Google Drive folder\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center><img src=\"imgs/dataset.png\"/></center>\n",
    "    <center><figcaption><b>CITE-seq dataset overview</b></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = pd.read_csv(DATASET_PATH/\"rna_scale.csv.gz\", index_col=0).T\n",
    "pro = pd.read_csv(DATASET_PATH/\"protein_scale.csv.gz\", index_col=0).T\n",
    "\n",
    "ncells = rna.shape[0]\n",
    "nfeatures_rna = rna.shape[1]\n",
    "nfeatures_pro = pro.shape[1]\n",
    "\n",
    "print(\"Number of cells:\", ncells)\n",
    "print(\"Number of geres:\", nfeatures_rna)\n",
    "print(\"Number of proteins:\", nfeatures_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat rna and pro\n",
    "print(\"RNA and protein cell barcodes match:\", all(rna.index == pro.index))\n",
    "citeseq = pd.concat([rna, pro], axis=1)\n",
    "citeseq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations\n",
    "metadata = pd.read_csv(DATASET_PATH/\"metadata.csv.gz\", index_col=0)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate CD4 and CD8 in l1\n",
    "metadata[\"celltype.l1.5\"] = metadata[\"celltype.l1\"].values\n",
    "metadata.loc[metadata[\"celltype.l2\"].str.startswith(\"CD4\"), \"celltype.l1.5\"] = \"CD4 T\"\n",
    "metadata.loc[metadata[\"celltype.l2\"].str.startswith(\"CD8\"), \"celltype.l1.5\"] = \"CD8 T\"\n",
    "metadata.loc[metadata[\"celltype.l2\"]==\"Treg\", \"celltype.l1.5\"] = \"CD4 T\"\n",
    "metadata.loc[metadata[\"celltype.l2\"]==\"MAIT\", \"celltype.l1.5\"] = \"MAIT\"\n",
    "metadata.loc[metadata[\"celltype.l2\"]==\"gdT\", \"celltype.l1.5\"] = \"gdT\"\n",
    "print(\"CITE-seq data and metadata cell barcodes match:\", all(citeseq.index == pro.index))\n",
    "\n",
    "# convert to categorical\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels = le.fit_transform(metadata[\"celltype.l1.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Custome dataset for tabular data\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, labels: np.ndarray):\n",
    "        self.data = torch.tensor(df.to_numpy(), dtype=torch.float)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset(citeseq, labels)\n",
    "\n",
    "# train, validation, and test split\n",
    "train_size = int(ncells*0.7)\n",
    "val_size = int(ncells*0.15)\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, ncells-train_size-val_size],\n",
    "                                         generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of cells for training:\", len(train_ds))\n",
    "print(\"Number of cells for validation:\", len(val_ds))\n",
    "print(\"Number of cells for test:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, drop_last=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_dl.dataset[0]\n",
    "print(\"Input data:\", x)\n",
    "print(\"Label:     \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use autoencoders for single-cell analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center><img src=\"imgs/autoencoder.png\"/></center>\n",
    "    <center><figcaption>Image source: Eraslan et al. Nat Rev Genet. 2019</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center><img src=\"imgs/autoencoder_arch.png\"/></center>\n",
    "    <center><figcaption><b>Autoencoder architecture for CITE-seq data</b></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinBnDrop(nn.Sequential):\n",
    "    \"\"\"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers, adapted from fastai.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=True):\n",
    "        layers = [nn.BatchNorm1d(n_out if lin_first else n_in)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        if act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder for CITE-seq data\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nfeatures_rna: int,\n",
    "                 nfeatures_pro: int,\n",
    "                 hidden_rna: int,\n",
    "                 hidden_pro: int,\n",
    "                 latent_dim: int,\n",
    "                 p: float = 0):\n",
    "        super().__init__()\n",
    "        self.nfeatures_rna = nfeatures_rna\n",
    "        self.nfeatures_pro = nfeatures_pro\n",
    "        hidden_dim = hidden_rna + hidden_pro\n",
    "        \n",
    "        self.encoder_rna = nn.Sequential(\n",
    "            LinBnDrop(nfeatures_rna, nfeatures_rna // 2, p=p, act=nn.LeakyReLU()),\n",
    "            LinBnDrop(nfeatures_rna // 2, hidden_rna, act=nn.LeakyReLU())\n",
    "        )\n",
    "        self.encoder_protein = LinBnDrop(nfeatures_pro, hidden_pro, p=p, act=nn.LeakyReLU())\n",
    "        self.encoder = LinBnDrop(hidden_dim, latent_dim, act=nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_rna = self.encoder_rna(x[:, :self.nfeatures_rna])\n",
    "        x_pro = self.encoder_protein(x[:, self.nfeatures_rna:])\n",
    "        x = torch.cat([x_rna, x_pro], 1)\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder for CITE-seq data\"\"\"\n",
    "    def __init__(self,\n",
    "                 nfeatures_rna: int,\n",
    "                 nfeatures_pro: int,\n",
    "                 hidden_rna: int,\n",
    "                 hidden_pro: int,\n",
    "                 latent_dim: int):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_rna + hidden_pro\n",
    "        out_dim = nfeatures_rna + nfeatures_pro\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            LinBnDrop(latent_dim, hidden_dim, act=nn.LeakyReLU()),\n",
    "            LinBnDrop(hidden_dim, out_dim // 2, act=nn.LeakyReLU()),\n",
    "            LinBnDrop(out_dim // 2, out_dim, bn=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteAutoencoder(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 nfeatures_rna: int,\n",
    "                 nfeatures_pro: int,\n",
    "                 hidden_rna: int,\n",
    "                 hidden_pro: int,\n",
    "                 latent_dim: int,\n",
    "                 p: float = 0,\n",
    "                 lr: float = 0.1):\n",
    "        \"\"\" Autoencoder for citeseq data \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    " \n",
    "        self.encoder = Encoder(nfeatures_rna, nfeatures_pro, hidden_rna, hidden_pro, latent_dim, p)\n",
    "        self.decoder = Decoder(nfeatures_rna, nfeatures_pro, hidden_rna, hidden_pro, latent_dim)\n",
    "        \n",
    "        # example input array for visualizing network graph\n",
    "        self.example_input_array = torch.zeros(256, nfeatures_rna + nfeatures_pro)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # extract latent embeddings\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "    \n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\" Calculate MSE loss for a given batch. \"\"\"\n",
    "        x, _ = batch\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        # MSE loss\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_citeseq(latent_dim: int):\n",
    "    trainer = pl.Trainer(default_root_dir=CHECKPOINT_PATH,\n",
    "                         gpus=1 if \"cuda\" in str(device) else 0,\n",
    "                         max_epochs=50,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    trainer.logger._log_graph = True\n",
    "    trainer.logger._default_hp_metric=None\n",
    "    \n",
    "    model = CiteAutoencoder(nfeatures_rna,\n",
    "                            nfeatures_pro,\n",
    "                            hidden_rna=30,\n",
    "                            hidden_pro=18,\n",
    "                            latent_dim=latent_dim,\n",
    "                            p=0.1,\n",
    "                            lr=0.1)\n",
    "    trainer.fit(model, train_dl, val_dl)\n",
    "     \n",
    "    val_result = trainer.test(model, val_dl, verbose=False)\n",
    "    test_result = trainer.test(model, test_dl, verbose=False)\n",
    "    result = {\"test\": test_result, \"val\": val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, result = train_citeseq(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --port 6006 --logdir saved_models/lightning_logs/version_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = []\n",
    "test_labels = []\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():    \n",
    "    for x, y in tqdm(test_dl, desc=\"Encoding cells\"):\n",
    "        test_encodings.append(model(x.to(model.device)))\n",
    "        test_labels += y.to(torch.int).tolist()\n",
    "        \n",
    "test_embeds = torch.cat(test_encodings, dim=0).cpu().numpy()\n",
    "test_labels = le.inverse_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run umap for dimensionality reduction and visualization\n",
    "embeds_umap = umap.UMAP(random_state=0).fit_transform(test_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize umap\n",
    "fig = px.scatter(x=embeds_umap[:, 0], y=embeds_umap[:, 1], color=test_labels, width=800, height=600,\n",
    "                 labels={\n",
    "                     \"x\": \"UMAP1\",\n",
    "                     \"y\": \"UMAP2\",\n",
    "                     \"color\": \"Cell type\"}\n",
    "                )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization with tensorboard\n",
    "writer = SummaryWriter(\"tensorboard/\")\n",
    "writer.add_embedding(test_embeds, metadata=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --port 6007 --logdir  tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
